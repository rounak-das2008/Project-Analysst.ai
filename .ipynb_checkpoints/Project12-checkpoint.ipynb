{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b08b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2665fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Name\n",
    "def get_name(soup):\n",
    "\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        title_value = title.text\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'class':'a-price'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"span\", attrs={'class':'a-offscreen'}).string.strip()\n",
    "\n",
    "        except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\n",
    "\n",
    "    return review_count\n",
    "\n",
    "\n",
    "#Function to extract Description\n",
    "def get_description(soup):\n",
    "    try:\n",
    "        description = soup.find(\"ul\", attrs={\"class\": \"a-unordered-list a-vertical a-spacing-mini\"}).text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        description = \"\"\n",
    "        \n",
    "    return description\n",
    "\n",
    "#Function to extract Product Details\n",
    "def get_details(soup):\n",
    "    try:\n",
    "        details = \"\"\n",
    "        details_div = soup.find(\"div\", attrs={\"id\": \"detailBullets_feature_div\"})\n",
    "        details_li = details_div.find_all('li')\n",
    "        for product_detail in details_li:\n",
    "            pd = product_detail.find_all('span', attrs={'class':'a-list-item'})\n",
    "            for pd_i in pd:\n",
    "                pd_i_text = pd_i.find_all('span')\n",
    "                pd_i_d, pd_i_val = pd_i_text[0].text.split('  ')[0].strip(), pd_i_text[1].text.strip()\n",
    "                details += pd_i_d + \": \" + pd_i_val + \"\\n\"\n",
    "          \n",
    "    except AttributeError:\n",
    "        details = \"\"\n",
    "        \n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76256374",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    baseURL = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%252+C283&ref=sr_pg_1\"\n",
    "    \n",
    "    d = {\"Name\":[], \"URL\":[], \"Price\":[], \"Rating\":[], \"Reviews\":[],\"Description\":[], \"ASIN\":[], \"Details\":[],\"Manufacturer\":[]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Loop for extracting 200 products\n",
    "    while(count<200):\n",
    "        \n",
    "        # HTTP Request\n",
    "        if(i==0):\n",
    "            webpage = requests.get(baseURL, headers=HEADERS)\n",
    "        else:\n",
    "            try:\n",
    "                new_baseURL = \"https://www.amazon.in/s?k=bags&page={}&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%252+C283&ref=sr_pg_{}\".format(i,i)\n",
    "                webpage = requests.get(new_baseURL, headers=HEADERS)\n",
    "            except:\n",
    "                print(\"Connection Error! Sleeping for 5 sec ... \")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "        # Fetching links as List of Tag Objects\n",
    "        links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "\n",
    "        # Storing the links\n",
    "        links_list = []\n",
    "\n",
    "        # Loop for extracting links from Tag Objects\n",
    "        for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "                \n",
    "        count += len(links_list)\n",
    "        \n",
    "\n",
    "        # Loop for extracting product details from each link \n",
    "        for link in links_list:\n",
    "            new_url = \"https://www.amazon.in\" + link\n",
    "            try:\n",
    "                new_webpage = requests.get(new_url, headers=HEADERS)\n",
    "                new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "            except:\n",
    "                print(\"Connection Error! Sleeping for 5 seconds ...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            details = get_details(new_soup)\n",
    "            \n",
    "            prod_asin = \"Not found\"\n",
    "            prod_manufacturer = \"Not found\"\n",
    "            for detail in details.split(\"\\n\"):\n",
    "                if \"ASIN\" in detail:\n",
    "                    try:\n",
    "                        prod_asin = detail.split(\":\")[1].strip()\n",
    "                    except:\n",
    "                        prod_asin = \"\"\n",
    "                if \"Manufacturer\" in detail:\n",
    "                    try:\n",
    "                        prod_manufacturer = detail.split(\":\")[1].strip()\n",
    "                    except:\n",
    "                        prod_manufacturer = \"\"\n",
    "\n",
    "            d['Name'].append(get_name(new_soup))\n",
    "            d['URL'].append(new_url)\n",
    "            d['Price'].append(get_price(new_soup))\n",
    "            d['Rating'].append(get_rating(new_soup))\n",
    "            d['Reviews'].append(get_review_count(new_soup))\n",
    "            d['Description'].append(get_description(new_soup))\n",
    "            d['ASIN'].append(prod_asin)\n",
    "            d['Details'].append(details)\n",
    "            d['Manufacturer'].append(prod_manufacturer)\n",
    "\n",
    "    \n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    amazon_df['Name'].replace('', np.nan, inplace=True)\n",
    "    amazon_df = amazon_df.dropna(subset=['Name'])\n",
    "    amazon_df.to_csv(\"AmazonProductData.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec245b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33443",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82801c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.iloc[0].Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.iloc[0].Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a80b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.iloc[13].Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b516202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
